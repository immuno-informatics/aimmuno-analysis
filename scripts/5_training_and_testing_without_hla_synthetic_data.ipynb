{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, roc_curve\n",
    "import time\n",
    "\n",
    "from tape import ProteinBertModel, TAPETokenizer\n",
    "\n",
    "random_seed = 22\n",
    "\n",
    "_ = torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define paths and experiment name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = \"/path/to/project/root\"\n",
    "\n",
    "data_dir = project_dir + \"/data\"\n",
    "models_dir = data_dir + \"/models\"\n",
    "temp_dir = project_dir + \"/temp\"\n",
    "datasets_dir = data_dir + \"/datasets\"\n",
    "\n",
    "dataset_easy_true_train_file = datasets_dir + \"/synthetic_easy/true_dataset_train.csv\"\n",
    "dataset_easy_true_test_file = datasets_dir + \"/synthetic_easy/true_dataset_test.csv\"\n",
    "dataset_easy_decoys_train_file = datasets_dir + \"/synthetic_easy/decoys_dataset_train.csv\"\n",
    "dataset_easy_decoys_test_file = datasets_dir + \"/synthetic_easy/decoys_dataset_test.csv\"\n",
    "\n",
    "dataset_hard_true_train_file = datasets_dir + \"/synthetic_hard/true_dataset_train.csv\"\n",
    "dataset_hard_true_test_file = datasets_dir + \"/synthetic_hard/true_dataset_test.csv\"\n",
    "dataset_hard_decoys_train_file = datasets_dir + \"/synthetic_hard/decoys_dataset_train.csv\"\n",
    "dataset_hard_decoys_test_file = datasets_dir + \"/synthetic_hard/decoys_dataset_test.csv\"\n",
    "\n",
    "dataset_src_col_pep = \"peptide\"\n",
    "dataset_src_col_label = \"label\"\n",
    "dataset_src_col_hla_seq = \"HLA_sequence\"\n",
    "\n",
    "dataset_true_col_pep = \"peptide\"\n",
    "dataset_true_col_hla_seq = \"mhc\"\n",
    "dataset_true_col_label = \"label\"\n",
    "\n",
    "dataset_decoys_col_pep = \"peptide_rand\"\n",
    "dataset_decoys_col_hla_seq = \"mhc_rand\"\n",
    "dataset_decoys_col_label = \"label\"\n",
    "\n",
    "model_type = \"tape\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select experiment type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"synthetic_easy\"\n",
    "\n",
    "# exp_name = \"synthetic_hard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S-%f\")\n",
    "\n",
    "weights_file = models_dir + f\"/model_{model_type}_{exp_name}_no_hla.pt\"\n",
    "\n",
    "results_dir = data_dir + f\"/results/{model_type}_{exp_name}_no_hla\"\n",
    "\n",
    "train_results_file = results_dir + f\"/results_{model_type}_{exp_name}_no_hla_train.txt\"\n",
    "test_results_file = results_dir + f\"/results_{model_type}_{exp_name}_no_hla_test.txt\"\n",
    "\n",
    "temp_prediction_flat_file = (\n",
    "    temp_dir + f\"/model_training_test_prediction_{model_type}_{exp_name}_no_hla_flat_{time_stamp}\"\n",
    ")\n",
    "temp_prediction_prob_file = (\n",
    "    temp_dir + f\"/model_training_test_prediction_{model_type}_{exp_name}_no_hla_prob_{time_stamp}\"\n",
    ")\n",
    "\n",
    "roc_file = results_dir + f\"/test_roc_{model_type}_{exp_name}_no_hla.csv\"\n",
    "roc_col_fpr = \"False_positive_rate\"  # Column name\n",
    "roc_col_tpr = \"True_positive_rate\"  # Column name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "if not os.path.exists(temp_dir):\n",
    "    os.makedirs(temp_dir)\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp_name == \"synthetic_easy\":\n",
    "    dataset_true_train = pd.read_csv(dataset_easy_true_train_file, index_col=0)\n",
    "    dataset_decoys_train = pd.read_csv(dataset_easy_decoys_train_file, index_col=0)\n",
    "elif exp_name == \"synthetic_hard\":\n",
    "    dataset_true_train = pd.read_csv(dataset_hard_true_train_file, index_col=0)\n",
    "    dataset_decoys_train = pd.read_csv(dataset_hard_decoys_train_file, index_col=0)\n",
    "\n",
    "dataset_true_train = dataset_true_train.rename(columns={dataset_true_col_hla_seq: dataset_src_col_hla_seq})\n",
    "dataset_decoys_train = dataset_decoys_train[[dataset_decoys_col_pep, dataset_decoys_col_label, dataset_decoys_col_hla_seq]].rename(columns={dataset_decoys_col_pep: dataset_src_col_pep, dataset_decoys_col_hla_seq: dataset_src_col_hla_seq})\n",
    "\n",
    "df = pd.concat([dataset_true_train, dataset_decoys_train], ignore_index=True, axis=0)\n",
    "\n",
    "del dataset_true_train, dataset_decoys_train\n",
    "\n",
    "df[dataset_src_col_label] = df[dataset_src_col_label].replace({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Input tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peptide_sequences = df.peptide.values\n",
    "\n",
    "labels = df.label.values\n",
    "labels = labels.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the TAPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TAPETokenizer(vocab='iupac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tokenizing inputs\")\n",
    "\n",
    "tokenized_peptide = [tokenizer.encode(sent) for sent in peptide_sequences]\n",
    "tokenized_train = pad_sequences([sent for sent in tokenized_peptide],\n",
    "                                dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(tokenized_train, labels, random_state=1024, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert both of them into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert into dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_inputs, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=128)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_labels)\n",
    "validation_sampler = RandomSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TAPEClassification(nn.Module):\n",
    "    def __init__(self, input_dim_bert, output_dim):\n",
    "        super().__init__()\n",
    "        self.bert = ProteinBertModel.from_pretrained('bert-base')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(input_dim_bert, output_dim)\n",
    "\n",
    "    def forward(self, x_sem):\n",
    "        pooled_output = self.bert(x_sem)[0][:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        out = self.classifier(pooled_output)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TAPEClassification(768,2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define early stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, tolerance=2, min_delta=0.00002, model=None, store_position =None):\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.model = model\n",
    "        self.store_position = store_position\n",
    "        self.max_value = 0\n",
    "\n",
    "    def __call__(self, new_value):\n",
    "        if new_value - self.min_delta > self.max_value :\n",
    "            self.counter = 0\n",
    "\n",
    "            torch.save(self.model.state_dict(), self.store_position)\n",
    "            self.max_value = new_value\n",
    "        else:\n",
    "            self.counter +=1\n",
    "\n",
    "        if self.counter >= self.tolerance:\n",
    "            self.early_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "early_stopping = EarlyStopping(tolerance=2, min_delta=0.002, model = model, store_position = weights_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a function to calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_set = []\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "with open(train_results_file, 'w') as f:\n",
    "    f.write('===== TRAIN RESULTS =====\\n')\n",
    "\n",
    "for _ in tqdm(range(0, epochs), leave=False, desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    acc_flag = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    for batch in tqdm(train_dataloader, leave=False, desc=\"Training\"):\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        b_input_ids,  b_labels = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(b_input_ids)\n",
    "        loss = criterion(logits, b_labels)\n",
    "\n",
    "        train_loss_set.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        b_labels = b_labels.detach().cpu().numpy()\n",
    "        accuracy = flat_accuracy(logits, b_labels)\n",
    "        acc_flag+=accuracy\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    print(\"Train acc: {}\".format(acc_flag/nb_tr_steps))\n",
    "\n",
    "    train_loss_list.append(tr_loss/nb_tr_steps)\n",
    "    train_acc_list.append(acc_flag/nb_tr_steps)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(validation_dataloader, leave=False, desc=\"Validating\"):\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        b_input_ids, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids)\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "    val_acc_list.append(eval_accuracy/nb_eval_steps)\n",
    "    time.sleep(1)\n",
    "\n",
    "    with open(train_results_file, 'a') as f:\n",
    "        f.write('EPOCH '+str(len(train_loss_list))+' --> LOSS: '+ str(train_loss_list[-1])\n",
    "                +' TRAIN_ACC: '+ str(train_acc_list[-1])\n",
    "                + ' VAL_ACC: ' + str(val_acc_list[-1]))\n",
    "        f.write('\\n')\n",
    "\n",
    "    early_stopping(val_acc_list[-1])\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"We stop for early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Testing\n",
    "\n",
    "Load the fine-tuned weights (skip the following cell box if you run the experiment in one-shot mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(weights_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp_name == \"synthetic_easy\":\n",
    "    dataset_true_test = pd.read_csv(dataset_easy_true_test_file, index_col=0)\n",
    "    dataset_decoys_test = pd.read_csv(dataset_easy_decoys_test_file, index_col=0)\n",
    "elif exp_name == \"synthetic_hard\":\n",
    "    dataset_true_test = pd.read_csv(dataset_hard_true_test_file, index_col=0)\n",
    "    dataset_decoys_test = pd.read_csv(dataset_hard_decoys_test_file, index_col=0)\n",
    "\n",
    "dataset_true_test = dataset_true_test.rename(columns={dataset_true_col_hla_seq: dataset_src_col_hla_seq})\n",
    "dataset_decoys_test = dataset_decoys_test[[dataset_decoys_col_pep, dataset_decoys_col_label, dataset_decoys_col_hla_seq]].rename(columns={dataset_decoys_col_pep: dataset_src_col_pep, dataset_decoys_col_hla_seq: dataset_src_col_hla_seq})\n",
    "\n",
    "df_test = pd.concat([dataset_true_test, dataset_decoys_test], ignore_index=True, axis=0)\n",
    "\n",
    "del dataset_true_test, dataset_decoys_test\n",
    "\n",
    "df_test[dataset_src_col_label] = df_test[dataset_src_col_label].replace({True: 1, False: 0})\n",
    "\n",
    "test_peptide = df_test.peptide.values\n",
    "test_labels = df_test.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tokenizing inputs\")\n",
    "\n",
    "tokenized_test_peptide = [tokenizer.encode(sent) for sent in test_peptide]\n",
    "\n",
    "tokenized_test = pad_sequences([sent for sent in tokenized_test_peptide],\n",
    "                                dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert them into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = torch.tensor(tokenized_test)\n",
    "test_labels = torch.tensor(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TensorDataset(test_inputs, test_labels)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing\n",
    "\n",
    "Save the prediction to a dedicated file to avoid memory crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(temp_prediction_flat_file, \"w\") as f:\n",
    "    pass\n",
    "with open(temp_prediction_prob_file, \"w\") as f:\n",
    "    pass\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for batch in tqdm(test_dataloader, leave=False, desc=\"Testing\"):\n",
    "    predictions = []\n",
    "\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(b_input_ids)\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    with open(temp_prediction_prob_file, \"a\") as f:\n",
    "        for elem in logits[:, 1]:\n",
    "            f.write(str(elem) + \" \")\n",
    "\n",
    "    predictions.append(logits)\n",
    "\n",
    "    flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "    with open(temp_prediction_flat_file, \"a\") as f:\n",
    "        for elem in flat_predictions:\n",
    "            f.write(str(elem) + \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(temp_prediction_prob_file, \"r\") as f:\n",
    "    data = f.read()\n",
    "data_list = data.split(\" \")\n",
    "array_list = np.array(data_list[:-1])\n",
    "prob_predictions = array_list.astype(float)\n",
    "os.remove(temp_prediction_prob_file)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(test_labels, prob_predictions)\n",
    "\n",
    "roc = pd.DataFrame({roc_col_fpr: fpr, roc_col_tpr: tpr})\n",
    "roc.to_csv(roc_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(temp_prediction_flat_file, 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "data_list = data.split(' ')\n",
    "\n",
    "array_list = np.array(data_list[:-1])\n",
    "flat_predictions = array_list.astype(int)\n",
    "\n",
    "os.remove(temp_prediction_flat_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_results_file, 'w') as f:\n",
    "    f.write('===== TEST RESULTS =====\\n')\n",
    "    f.write('ACCURACY --> ' + str(accuracy_score(test_labels, flat_predictions)) +'\\n')\n",
    "    f.write('F1-MACRO --> ' + str(f1_score(test_labels, flat_predictions, average='macro'))+'\\n')\n",
    "    f.write('F1 WEIGHTED --> '+ str(f1_score(test_labels, flat_predictions, average='weighted'))+'\\n')\n",
    "    f.write('Precision TRUE_label --> ' + str(precision_score(test_labels, flat_predictions, average='binary'))+'\\n')\n",
    "    f.write('Recall TRUE_label --> ' + str(recall_score(test_labels, flat_predictions, average='binary'))+'\\n')\n",
    "    f.write('F1 TRUE_label --> ' + str(f1_score(test_labels, flat_predictions, average='binary'))+'\\n')\n",
    "    f.write('AUC ROC --> ' + str(roc_auc_score(test_labels, flat_predictions))+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
